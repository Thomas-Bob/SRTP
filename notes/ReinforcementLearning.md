# ReinforcementLearning笔记

## 理论部分

### 任务与奖赏

- 强化学习任务通常用马尔可夫决策过程(Markov Decision Process, 简称MDP)来描述:
  - 机器处于环境E中, 状态空间为X, 其中每个状态$x\in X$是机器感知到的环境的描述;
  - 机器能采取的动作构成了动作空间A;
  - 若某个动作$a\in A$作用在当前状态x上，则潜在的转移函数P将使得环境从当前状态按某种概率转移到另一个状态;
  - 在转移到另一个状态的同时，环境会根据潜在的"奖赏"(reward)函数R反馈给机器一个奖赏.
- 机器要做的是通过在环境中不断地尝试而学得一个"策略"(policy)$\pi$, 根据这个策略, 在状态x下就能得知要执行的动作$a=\pi(x)$
  - 将策略表示为函数$\pi$, 确定性策略常用这种表示
  - 概率表示$\pi$, 随机性策略常用这种表示, $\pi(x,a)$为状态x下选择动作a的概率.
- 在强化学习任务中, 学习的目的就是要找到能使长期累积奖赏最大化的策略
  - T步累积奖赏$E[\frac{1}{T}\sum_{t=1}^Tr_t]$
  - $\gamma$折扣累积奖赏$E[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}]$

### K-摇臂赌博机

- 探索与利用
  - **仅探索法**: 仅为获知每个摇臂的期望奖赏
  将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂), 最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计.
  - **仅利用法**: 仅为执行奖赏最大的动作
  按下目前最优的(即到目前为止平均奖赏最大的)摇臂, 若有多个摇臂同为最优, 则从中随机选取一个.
  - 尝试次数有限: 探索-利用窘境
- $\epsilon$-贪心
  - 每次尝试时, 以$\epsilon$的概率进行探索, 即以均匀概率随机选取一个摇臂; 以$1-\epsilon$的概率进行利用, 即选择当前平均奖赏最高的摇臂(若有多个, 则随机选取一个).
  $$
  Q_n(k)=Q_{n-1}(k)+\frac{1}{n}(v_n-Q_{n-1}(k))
  $$
- Softmax
  - 基于当前已知的摇臂平均奖赏来对探索和利用进行折中
  $$
  P(k)=\frac{e^{\frac{Q(k)}{\tau}}}{\sum\limits_{i=1}^Ke^{\frac{Q(i)}{\tau}}}
  $$
  其中, $\tau>0$成为"温度", $\tau$越小则平均奖赏高的摇臂被选取的概率越高. $\tau$趋于0时Softmax将趋于"仅利用", $\tau$趋于无穷大时Softmax则将趋于"仅探索".

### 有模型学习

对于任意状态x, x'和动作a, 在x状态下执行动作a转移到x'状态的概率$P_{x\rightarrow x'}^a$是已知的, 该转移所带来的奖赏$R_{x\rightarrow x'}^a$也是已知的.

- 策略评估
  - 函数$V^{\pi}(x)$表示从状态x出发, 使用策略$\pi$所带来的累积奖赏, 表示指定"状态"上的累积奖赏, 称为"状态值函数".
  - 函数$Q^{\pi}(x,a)$表示从状态x出发, 执行动作a后再使用策略$\pi$带来的累积奖赏, 表示指定"状态-动作"上的累积奖赏, 称为"状态-动作值函数".
  - 可以转化为动态规划算法
- 策略改进
  - 对某个策略的累积奖赏进行评估后, 若发现它并非最优策略, 则希望对其进行改进
  - 一个强化学习任务可能有多个最优策略, 最优策略所对应的值函数$V^*$称为最优值函数, 即
  $$
  \forall x\in X: V^*(x)=V^{\pi^*}(x)
  $$
  - 最优Bellman等式, 其唯一解是最优值函数
    - 揭示了非最优策略的改进方式: 将策略选择的动作改变为当前最优的动作
- 策略迭代与值迭代
  - **策略迭代**: 从一个初始策略(通常是随机策略)出发, 先进行策略评估, 然后改进策略, 评估改进的策略, 再进一步改进策略, ……不断迭代进行策略评估和改进, 直到策略收敛、不再改变为之.
  - 策略改进与值函数的改进是一致的, 因此可将策略改进视为值函数的改善, 于是可得到值迭代算法.
  - **在模型已知时强化学习任务能归结为基于动态规划的寻优问题**

### 免模型学习

- 蒙特卡罗强化学习
  - 策略迭代算法估计的是V, 而最终的策略是通过Q来获得. 当模型已知时, 从V到Q有很简单的转换方法, 于是将估计对象从V转变为Q.
  - **"同策略"蒙特卡罗强化学习算法**: 被评估与被改进的是同一个策略
  - **"异策略"蒙特卡罗强化学习算法**
- 时序差分学习(Temporal Difference, 简称TD)
  - 结合了动态规划与蒙特卡罗方法的思想
  - **Sarsa算法**: 同策略算法, 评估、执行的均为$\epsilon$-贪心策略
  - **Q-学习算法**: 异策略算法, 评估的是原始策略, 执行的是$\epsilon$-贪心策略

### 值函数近似

- 解决面临的状态空间是连续的, 有无穷多个状态的情况
- 值函数能表达为状态的线性函数
- 由于此时的值函数难以像有限状态那样精确记录每个状态的值, 因此这样值函数的求解被称为值函数近似

### 模仿学习

- 直接模仿学习
  - 直接模仿人类专家的"状态-动作对"
  - 把状态作为特征, 动作作为标记
- 逆强化学习
  - 设计奖赏函数困难, 从人类专家提供的范例数据中反推出奖赏函数
  - 寻找某种奖赏函数使得范例数据是最优的, 然后即可使用这个奖赏函数来训练强化学习策略

## 实践部分

在Julia中运用ReinforcementLearning库来运行一个简单的强化学习实验：

```julia
using ReinforcementLearning
run( # run函数是强化学习实验的入口点，它接受几个参数来定义实验的设置
    RandomPolicy(), # 这是一个策略(Policy)；RandomPolicy()表示一个随机策略，即智能体在每个时间步骤中随机选择一个可用的动作。这通常用于baseline性能测试，因为它不涉及任何学习过程。
    CartPoleEnv(), # 这是实验使用的环境(Environment)，这里是经典的倒立摆(CartPole)问题。在这个问题中，智能体的目标是通过左右移动底部的小车来保持上面的杆垂直不倒。CartPoleEnv()是一个模拟环境，用于评估智能体的策略表现。
    StopAfterNSteps(1_000), # 这是一个停止条件(Stop Criterion)，指定了实验运行的时间步骤数。在这个例子中，实验将在1000个时间步骤后停止，确保实验不会无限运行。
    TotalRewardPerEpisode() # 这是一个日志记录器(Logger)，用于记录每个回合(episode)结束时智能体获得的总奖励。这对于评估和比较不同策略的性能非常有用。
)
```
